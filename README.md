
# Reto: Conductas en Playground con Esqueletos 

## üìå Descripci√≥n

Construir un sistema que, a partir de **ventanas de video (3‚Äì5 s)** y **esqueletos 2D**, prediga **etiquetas multi-etiqueta** a nivel de **escena** en un playground:

* **Riesgo de uso:** `wrong_way_climbing`, `blocking_slide_exit`, `falling`
* **Interacci√≥n peligrosa:** `pushing_collision`, `adult_child_aggressive`
* **Social positiva:** `cooperative_play`, `adult_assisting`
* **Base:** `normal_play`

**Modelo core (obligatorio):** encoder temporal por persona ‚Üí **agregador por atenci√≥n** (set de personas) ‚Üí **cabeza multi-label**.
**Stretch (opcional):** integrar **MP-GCN** (grafo panor√°mico) o a√±adir nodos de **objetos** (tobog√°n/columpio/estructura).

---

## üß≠ Estructura sugerida del repo

```
project/
  README.md
  notebooks/
    00_query_roi_and_azure.ipynb   # detecciones‚ÜíROI‚ÜíAzure, descarga/listado
    01_make_windows.ipynb          # generar windows.csv (opcional si ya lo haces en 00)
    02_vlm_pseudolabels.ipynb      # auto-etiquetado (opcional)
  data/
    windows.csv                    # ventanas (t_start‚Äìt_end + blob_url_sas)
    train.csv                      # etiquetas (humanas o pseudo) + conf_weight
    val.csv
    npy/                           # una .npy por ventana [T,K_max,V,C]
  src/
    data/                          # loaders [T,K_max,V,C] + m√°scara
    model/                         # PersonEncoder, SceneAggregator, Head
    train/                         # loop de entrenamiento
    eval/                          # m√©tricas
    vlm/                           # agregador JSON‚ÜíCSV (CORE-CLEAN)
  configs/
    classes.yaml                   # minimal5 / full8
```

> Puedes empezar **solo** con `notebooks/00_query_roi_and_azure.ipynb` (ya incluido en tu repo): produce `windows.csv` y descarga clips desde Azure.

---

## üîê Configuraci√≥n (credenciales)

**No** subas contrase√±as ni SAS al repo. Usa variables de entorno o un `config.json` ignorado en git.

Variables t√≠picas:

```bash
# PostGIS
export POSTGIS_DB=crowdcounting
export POSTGIS_USER=...
export POSTGIS_PASS=...
export POSTGIS_HOST=...
export POSTGIS_PORT=5432

# Azure Blob (SAS solo lectura+lista)
export AZ_ACCOUNT_URL="https://<account>.blob.core.windows.net"
export AZ_CONTAINER="<container>"
export AZ_SAS="sv=...&sp=rl&sig=..."   # sp=rl (Read+List)
```

---

## üß± Dependencias m√≠nimas

```text
pandas
numpy
geopandas
psycopg2-binary
shapely
azure-storage-blob
tqdm
# Entrenamiento:
torch
scikit-learn
pyyaml
matplotlib
```

---

## üóÉÔ∏è Datos de entrada

* **PostGIS** con detecciones **ya filtradas por ROI** (√°rea de juegos).
  Tabla ejemplo:
  `person_observed(id, id_person, lat, long, timestamp, geom, camera_name, coordinate_x, coordinate_y, tracklet_id, ...)`

* **Azure Blob Storage** con los videos (container + **SAS de lectura**).

---

## üö¶ Pipeline reproducible

### 1) De detecciones a **segmentos con presencia** (ROI)

En `00_query_roi_and_azure.ipynb`:

1. Carga ROI (`playgroundROI.gpkg`, EPSG:4326).
2. Consulta PostGIS ‚Üí detecciones **dentro** del ROI + *timestamps*.
3. **Colapsa por segundo** (`timestamp.floor('S')`), cuenta personas (`nunique(tracklet_id)`).
4. Mant√©n segundos con `persons ‚â• 1`.
5. Une segundos contiguos en **segmentos** `[seg_start, seg_end)` (suma +1s al final).
6. Descarta segmentos cortos `< 3 s`.

> Esto puede hacerse con SQL (CTEs) o Pandas (ver notebook).

### 2) Segmentos ‚Üí **ventanas deslizantes** ‚Üí `windows.csv`

Dentro de cada segmento:

* Ventana `W = 5 s` (o `3 s`) con **hop** `= 2.5 s` (50%).
* **Salida:** `data/windows.csv` con columnas:

```csv
video_id,camera,t_start,t_end,blob_url_sas
C01_2025_09_10_120000,cam1,2025-09-10T12:00:00Z,2025-09-10T12:00:05Z,https://<acc>.blob.core.windows.net/<cont>/<path>.mp4?<SAS>
```

> `blob_url_sas` se arma con `AZ_ACCOUNT_URL`, `AZ_CONTAINER`, `blob_path` y `AZ_SAS`.

### 3) Ventanas ‚Üí **esqueletos** `.npy`

Para cada ventana (`t_start‚Äìt_end`):

* Extrae **T frames** uniformes (p.ej., `T=64‚Äì96`).
* Usa `K_max = 4‚Äì6` (m√°x. personas por ventana), `V=17` (COCO-17), `C=2` (x,y) o `3` (x,y,score).
* **Padding** con ceros para personas ausentes; el loader infiere **m√°scara**.
* **Normalizaci√≥n:** *root-centered* (cadera al origen) + *scale-invariant* (altura/torso).
* Guarda **una** `.npy` por ventana: **`[T, K_max, V, C]`** (float32).

> Si ya tienes pose/track por frame, **re√∫salos**; si no, usa un pose ligero (YOLOv8-pose, RTM/ViTPose-lite) y track *light*.

### 4) (Opcional) Auto-etiquetado con VLM ‚Üí **CORE-CLEAN**

Si no hay labels humanas:

1. Pasa ventanas a un **VLM** (2 vistas: RGB y *stick-figure*; 2 offsets).
2. Pide **JSON estricto** con probabilidades \[0..1] por etiqueta.
3. Ensemblado (promedio) + **CORE-CLEAN** (alta confianza/baja varianza).
4. Genera `data/train.csv` y `data/val.csv`:

```csv
npy_path,wrong_way_climbing,blocking_slide_exit,falling,pushing_collision,adult_child_aggressive,cooperative_play,adult_assisting,normal_play,conf_weight
data/npy/clip_0001.npy,0,0,1,0,0,0,0,0,0.95
```

**Prompt sugerido (multi-label, JSON estricto):**

```text
[SYSTEM] Responde SOLO con un JSON v√°lido.
[USER]
Analiza el clip. Devuelve {"scores":{etiqueta:prob,...}} con probabilidades [0..1] para:
["wrong_way_climbing","blocking_slide_exit","falling",
 "pushing_collision","adult_child_aggressive",
 "cooperative_play","adult_assisting","normal_play"]

Reglas:
- Si no hay tobog√°n visible: wrong_way_climbing y blocking_slide_exit ‚â§ 0.05.
- Si es dudoso: usa 0.0‚Äì0.2 (no inventes).
- No texto extra fuera del JSON.
```

### 5) Entrenamiento (Core)

**Arquitectura:**

* `PersonEncoder` (TemporalConv/LSTM peque√±a) ‚Üí `z_i ‚àà R^d` por persona.
* `SceneAggregator` (attention pooling o DeepSets con m√°scara) ‚Üí `z_scene`.
* `Head` (MLP multi-label) ‚Üí logits ‚Üí sigmoid.

**P√©rdida:** `BCEWithLogits` + *class-weights* (clases raras ‚Üë) + `sample_weight=conf_weight`.
**Hipers sugeridos:** `d=128`, `heads=4`, `layers=2`, `T=64`, `K_max=4‚Äì6`, batch `8‚Äì16`, LR `1e-3 ‚Üí 1e-4`, dropout `0.1`.
**M√©tricas:** mAP, F1 macro; **Recall** en `falling`/`adult_child_aggressive`; **FPR ‚â§ 5%** en clases de riesgo.
**Calibraci√≥n:** umbrales por clase (val) + **suavizado temporal** (EMA) para ventanas solapadas.

### 6) Stretch (opcional)

* **Objetos**: a√±adir centroides de `slide/swing` como ‚Äúpersonas especiales‚Äù (ocupan slots en `K_max`).
* **MP-GCN**: migrar al *panoramic graph* (streams J/B/JM/BM + atenci√≥n persona-tiempo).
* **Few-shot**: 50‚Äì100 ventanas por clase rara.
* **Auto-training**: re-entrenar la **head** con predicciones ‚â• 0.90 no vistas.

---

## üß© Ontolog√≠as

`configs/classes.yaml` sugerido:

```yaml
full8:
  - wrong_way_climbing
  - blocking_slide_exit
  - falling
  - pushing_collision
  - adult_child_aggressive
  - cooperative_play
  - adult_assisting
  - normal_play

minimal5:
  - falling
  - wrong_way_climbing
  - pushing_collision
  - cooperative_play
  - normal_play
```

---

## ‚öíÔ∏è Comandos √∫tiles (Azure / AzCopy)

```bash
# Listar blobs
azcopy ls "$AZ_ACCOUNT_URL/$AZ_CONTAINER?$AZ_SAS"

# Descargar un prefijo
azcopy copy "$AZ_ACCOUNT_URL/$AZ_CONTAINER/playground/2025/*?$AZ_SAS" ./videos/ --recursive=true

# Sincronizar (reanudar descargas)
azcopy sync "$AZ_ACCOUNT_URL/$AZ_CONTAINER/playground/2025/?$AZ_SAS" ./videos/ --recursive=true
```

---

## üóìÔ∏è Calendario (10 semanas, gu√≠a)

1. **Exploraci√≥n & datos** (ROI‚Üídetecciones‚Üísegmentos).
2. **Ventanas** (`windows.csv`) y set-up de `.npy`.
3. **PersonEncoder + Aggregator + Head** (forward OK).
4. **VLM pseudo-labels ‚Üí CORE-CLEAN** ‚Üí `train/val.csv`.
5. **Entrenamiento baseline** + reporte preliminar.
6. **Calibraci√≥n + suavizado**.
7. **Stretch:** objetos o **MP-GCN** (si hay tiempo).
8. **Few-shot** (opcional).
9. **Robustez & ablations** (por c√°mara, #personas; J vs J+B+JM+BM; con/sin objetos).
10. **Demo final** (Streamlit) + curvas PR/ROC + reporte.

---

## üéØ Metas y r√∫brica (sugeridas)

* **Semana 5:** F1 macro ‚â• **0.45** (val).
* **Semana 8:** Recall `falling` ‚â• **0.60** con **FPR ‚â§ 5%** (riesgos).
* **Semana 10:** demo fluida, **curvas PR/ROC**, ablations m√≠nimas y discusi√≥n de limitaciones.

**R√∫brica:**

* Infra & datos (20%) ‚Äî loaders reproducibles, normalizaci√≥n, notebook de exploraci√≥n.
* Baseline & training (35%) ‚Äî entrenamiento estable, m√©tricas razonables, manejo de desbalance.
* Calibraci√≥n & evaluaci√≥n (25%) ‚Äî umbrales por clase, FPR bajo, splits por c√°mara.
* Demo & reporte (20%) ‚Äî app clara, visualizaciones, ablations y futuro.

---

## üîó Referencias

* **MP-GCN (Panoramic Graph):**
  Paper: *Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph* (arXiv:2407.19497)
  Repo: [https://github.com/mgiant/MP-GCN](https://github.com/mgiant/MP-GCN)

* **ST-GCN (cl√°sico) y variantes:** para quien quiera profundizar en GCN de esqueletos.

---

## ‚ùìFAQ

**¬øSin etiquetas humanas?**
Usa VLM ‚Üí **CORE-CLEAN** para generar `train.csv` con `conf_weight`.

**¬øGPU peque√±a?**
`T=64`, `K_max=4`, `d=128`, batch `8‚Äì12`, ontolog√≠a `minimal5`.

**¬øPuedo aprobar sin objetos ni grafo?**
S√≠. El **Core** (encoder por persona + agregador + head) es suficiente; el grafo es **stretch**.

---

> **Checklist d√≠a 1‚Äì2**
>
> 1. Ejecuta `00_query_roi_and_azure.ipynb` y genera **segmentos** ‚Üí **ventanas** ‚Üí `windows.csv`.
> 2. Crea `.npy` por ventana (`[T,K_max,V,C]`) normalizados.
> 3. (Opcional) VLM ‚Üí CORE-CLEAN ‚Üí `train/val.csv`.
> 4. Entrena el **baseline** (5‚Äì8 √©pocas) y reporta F1/Recall/FPR.

---
